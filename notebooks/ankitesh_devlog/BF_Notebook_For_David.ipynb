{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(1,\"/home1/07064/tg863631/anaconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages\") #work around for h5py\n",
    "from cbrain.imports import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow_probability as tfp\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "# import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "# from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "# from climate_invariant import *\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from climate_invariant_utils import *\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates (just pick any file from the climate model run)\n",
    "coor = xr.open_dataset(\"/oasis/scratch/comet/ankitesh/temp_project/data/sp8fbp_minus4k.cam2.h1.0000-01-01-00000.nc\",\\\n",
    "                    decode_times=False)\n",
    "lat = coor.lat; lon = coor.lon; lev = coor.lev;\n",
    "coor.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINDIR = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/CRHData/'\n",
    "path = '/home/ankitesh/CBrain_project/CBRAIN-CAM/cbrain/'\n",
    "path_hyam = 'hyam_hybm.pkl'\n",
    "\n",
    "hf = open(path+path_hyam,'rb')\n",
    "hyam,hybm = pickle.load(hf)\n",
    "scale_dict = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGeneratorClimInv(DataGenerator):\n",
    "    \n",
    "    def __init__(self, data_fn, input_vars, output_vars,\n",
    "             norm_fn=None, input_transform=None, output_transform=None,\n",
    "             batch_size=1024, shuffle=True, xarray=False, var_cut_off=None, normalize_flag=True,\n",
    "             rh_trans=True,t2tns_trans=True,\n",
    "             lhflx_trans=True,\n",
    "             scaling=True,interpolate=True,\n",
    "             hyam=None,hybm=None,                 \n",
    "             inp_subRH=None,inp_divRH=None,\n",
    "             inp_subTNS=None,inp_divTNS=None,\n",
    "             lev=None, interm_size=40,\n",
    "             lower_lim=6,\n",
    "             is_continous=True,Tnot=5,\n",
    "                mode='train'):\n",
    "        \n",
    "        self.scaling = scaling\n",
    "        self.interpolate = interpolate\n",
    "        self.rh_trans = rh_trans\n",
    "        self.t2tns_trans = t2tns_trans\n",
    "        self.lhflx_trans = lhflx_trans\n",
    "        self.inp_shape = 64\n",
    "        self.mode=mode\n",
    "        super().__init__(data_fn, input_vars,output_vars,norm_fn,input_transform,output_transform,\n",
    "                        batch_size,shuffle,xarray,var_cut_off,normalize_flag) ## call the base data generator\n",
    "        self.inp_sub = self.input_transform.sub\n",
    "        self.inp_div = self.input_transform.div\n",
    "        if rh_trans:\n",
    "            self.qv2rhLayer = QV2RHNumpy(self.inp_sub,self.inp_div,inp_subRH,inp_divRH,hyam,hybm)\n",
    "        \n",
    "        if lhflx_trans:\n",
    "            self.lhflxLayer = LhflxTransNumpy(self.inp_sub,self.inp_div,hyam,hybm)\n",
    "            \n",
    "        if t2tns_trans:\n",
    "            self.t2tnsLayer = T2TmTNSNumpy(self.inp_sub,self.inp_div,inp_subTNS,inp_divTNS,hyam,hybm)\n",
    "            \n",
    "        if scaling:\n",
    "            self.scalingLayer = ScalingNumpy(hyam,hybm)\n",
    "            self.inp_shape += 1\n",
    "                    \n",
    "        if interpolate:\n",
    "            self.interpLayer = InterpolationNumpy(lev,is_continous,Tnot,lower_lim,interm_size)\n",
    "            self.inp_shape += interm_size*2 + 4 + 30 ## 4 same as 60-64 and 30 for lev_tilde.size\n",
    "        \n",
    "            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        # Grab batch from data\n",
    "        batch = self.data_ds['vars'][start_idx:end_idx]\n",
    "\n",
    "        # Split into inputs and outputs\n",
    "        X = batch[:, self.input_idxs]\n",
    "        Y = batch[:, self.output_idxs]\n",
    "        # Normalize\n",
    "        X_norm = self.input_transform.transform(X)\n",
    "        Y = self.output_transform.transform(Y)\n",
    "        X_result = X_norm\n",
    "        \n",
    "        if self.rh_trans:\n",
    "            X_result = self.qv2rhLayer.process(X_result) \n",
    "            \n",
    "        if self.lhflx_trans:\n",
    "            X_result = self.lhflxLayer.process(X_result)\n",
    "            X_result = X_result[:,:64]\n",
    "            X = X[:,:64]\n",
    "            \n",
    "        if self.t2tns_trans:\n",
    "            X_result = self.t2tnsLayer.process(X_result)\n",
    "        \n",
    "        if self.scaling:\n",
    "            scalings = self.scalingLayer.process(X) \n",
    "            X_result = np.hstack((X_result,scalings))\n",
    "        \n",
    "        if self.interpolate:\n",
    "            interpolated = self.interpLayer.process(X,X_result)\n",
    "            X_result = np.hstack((X_result,interpolated))\n",
    "            \n",
    "\n",
    "        if self.mode=='val':\n",
    "            return xr.DataArray(X_result), xr.DataArray(Y)\n",
    "        return X_result,Y\n",
    "    \n",
    "    ##transforms the input data into the required format, take the unnormalized dataset\n",
    "    def transform(self,X):\n",
    "        X_norm = self.input_transform.transform(X)\n",
    "        X_result = X_norm\n",
    "        \n",
    "        if self.rh_trans:\n",
    "            X_result = self.qv2rhLayer.process(X_result)  \n",
    "        \n",
    "        if self.lhflx_trans:\n",
    "            X_result = self.lhflxLayer.process(X_result)\n",
    "            X_result = X_result[:,:64]\n",
    "            X = X[:,:64]\n",
    "\n",
    "        if self.t2tns_trans:\n",
    "            X_result = self.t2tnsLayer.process(X_result)\n",
    "        \n",
    "        if self.scaling:\n",
    "            scalings = self.scalingLayer.process(X) \n",
    "            X_result = np.hstack((X_result,scalings))\n",
    "        \n",
    "        if self.interpolate:\n",
    "            interpolated = self.interpLayer.process(X,X_result)\n",
    "            X_result = np.hstack((X_result,interpolated))\n",
    "            \n",
    "\n",
    "        return X_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict_RH = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling.pkl')\n",
    "scale_dict_RH['RH'] = 0.01*L_S/G, # Arbitrary 0.1 factor as specific humidity is generally below 2%\n",
    "\n",
    "in_vars_RH = ['RH','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "\n",
    "TRAINFILE_RH = 'CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_RH = 'CI_RH_M4K_NORM_norm.nc'\n",
    "VALIDFILE_RH = 'CI_RH_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_RH = DataGenerator(\n",
    "    data_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using TNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TfromNS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "\n",
    "TRAINFILE_TNS = 'CI_TNS_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_TNS = 'CI_TNS_M4K_NORM_norm.nc'\n",
    "VALIDFILE_TNS = 'CI_TNS_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_TNS = DataGenerator(\n",
    "    data_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE = 'CI_SP_M4K_train_shuffle.nc'\n",
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'CI_SP_M4K_valid.nc'\n",
    "\n",
    "train_gen = DataGeneratorClimInv(\n",
    "    data_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+TRAINFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False\n",
    ")\n",
    "\n",
    "valid_gen = DataGeneratorClimInv(\n",
    "    data_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(64, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "path_HDF5 = '/oasis/scratch/comet/ankitesh/temp_project/models/'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+'BF_temp.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  163/41376 [..............................] - ETA: 15:16 - loss: 2141.5711"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0f35d231cc48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n\u001b[0;32m----> 3\u001b[0;31m               callbacks=[earlyStopping, mcp_save_pos])\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/advanced_activations.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(x, alpha, max_value, threshold)\u001b[0m\n\u001b[1;32m   4334\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4336\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4338\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(features, alpha, name)\u001b[0m\n\u001b[1;32m   2820\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m       \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(features, alpha, name)\u001b[0m\n\u001b[1;32m   5412\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5413\u001b[0m         \u001b[0;34m\"LeakyRelu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"alpha\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5414\u001b[0;31m         alpha)\n\u001b[0m\u001b[1;32m   5415\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5416\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chech on m4K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345.92472624813973"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(path_HDF5+'BF.hdf5')\n",
    "model.evaluate_generator(valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on p4k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "valid_gen = DataGeneratorClimInv(\n",
    "    data_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14576.757789136107"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(path_HDF5+'BF.hdf5')\n",
    "model.evaluate_generator(valid_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CbrainCustomLayer",
   "language": "python",
   "name": "cbraincustomlayer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
